{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1YYbH7hSWuU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = pd.read_parquet('/content/cleaned_hospital_readmission.parquet', engine='pyarrow')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.readmitted.copy()\n",
        "col_to_use = [\n",
        "    'race', 'gender', 'age', 'time_in_hospital', 'num_medications',\n",
        "    'number_outpatient', 'number_emergency', 'number_inpatient',\n",
        "    'number_diagnoses', 'metformin', 'repaglinide', 'nateglinide',\n",
        "    'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide',\n",
        "    'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin',\n",
        "    'glyburide-metformin', 'change', 'diabetesMed', 'admission_type',\n",
        "    'admission_sources', 'discharge_dispositions', 'primary_diagnosis_1',\n",
        "    'primary_diagnosis_2', 'primary_diagnosis_3'\n",
        "]\n",
        "X = df[col_to_use].copy()"
      ],
      "metadata": {
        "id": "Fq_0Y5k4UOCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = []\n",
        "categorical_cols =[]\n",
        "for x in X.columns:\n",
        "    if df[x].dtype in ['object', 'category']:\n",
        "        categorical_cols.append(x)\n",
        "    elif df[x].dtype == 'int64':\n",
        "        numerical_cols.append(x)\n",
        "\n",
        "\n",
        "# now we label those that were readmitted within 30 days as 1 and all other as zero\n",
        "y = y.apply(lambda x: 1 if x == '<30' else 0)\n",
        "\n",
        "y.value_counts(normalize=True), y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysXVLt30UXXo",
        "outputId": "1885d24a-34d3-49c9-d454-75b53f116a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(readmitted\n",
              " 0    0.884783\n",
              " 1    0.115217\n",
              " Name: proportion, dtype: float64,\n",
              " (95672,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
      ],
      "metadata": {
        "id": "-Lgg7FQnVDn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "hoWXiWUJU8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(kernel='rbf', C=1, gamma='scale', probability=True))\n",
        "])\n"
      ],
      "metadata": {
        "id": "Kvb9ET97Uaos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random 1% sample of the entire dataset\n",
        "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.90, random_state=42, stratify=y)\n",
        "\n",
        "# Now, split this small dataset into train and test sets\n",
        "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample)\n"
      ],
      "metadata": {
        "id": "tvDJKf_B95DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yi61dOpLMUvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for SVM\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'classifier__gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient\n",
        "    'classifier__kernel': ['rbf']  # Using RBF kernel\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(svm_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Train on the reduced dataset\n",
        "grid_search.fit(X_train_small, y_train_small)\n",
        "\n",
        "# Best parameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zzhmcft-WX-",
        "outputId": "ebeb4f4a-e74b-4085-f9b2-f931912088c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'classifier__C': 100, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
      ],
      "metadata": {
        "id": "y5H0VRGuF0t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_small)\n",
        "\n"
      ],
      "metadata": {
        "id": "UR3QuBpFFuTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmkIgQCxKTQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = best_model.predict(X_test_small)\n",
        "\n",
        "# Calculate Metrics\n",
        "accuracy = accuracy_score(y_test_small, y_pred)\n",
        "precision = precision_score(y_test_small, y_pred)\n",
        "recall = recall_score(y_test_small, y_pred)\n",
        "f1 = f1_score(y_test_small, y_pred)\n",
        "f2 = fbeta_score(y_test_small, y_pred, beta=2)  # F2-score (recall-weighted)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"F2 Score: {f2:.4f}\")\n",
        "\n",
        "# Print Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test_small, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_small, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbzKuYmfGJYY",
        "outputId": "3bf5b5ed-f680-4c23-9710-5e7cd20424b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8312\n",
            "Precision: 0.1634\n",
            "Recall: 0.1136\n",
            "F1 Score: 0.1340\n",
            "F2 Score: 0.1210\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1566  128]\n",
            " [ 195   25]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91      1694\n",
            "           1       0.16      0.11      0.13       220\n",
            "\n",
            "    accuracy                           0.83      1914\n",
            "   macro avg       0.53      0.52      0.52      1914\n",
            "weighted avg       0.81      0.83      0.82      1914\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, classification_report, confusion_matrix\n",
        "\n",
        "df_sampled = df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# Define features & target variable\n",
        "col_to_use = [\n",
        "    'race', 'gender', 'age', 'time_in_hospital', 'num_medications',\n",
        "    'number_outpatient', 'number_emergency', 'number_inpatient',\n",
        "    'number_diagnoses', 'metformin', 'repaglinide', 'nateglinide',\n",
        "    'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide',\n",
        "    'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin',\n",
        "    'glyburide-metformin', 'change', 'diabetesMed', 'admission_type',\n",
        "    'admission_sources', 'discharge_dispositions', 'primary_diagnosis_1',\n",
        "    'primary_diagnosis_2', 'primary_diagnosis_3'\n",
        "]\n",
        "\n",
        "# Extract features & target\n",
        "X = df_sampled[col_to_use].copy()\n",
        "y = df_sampled['readmitted'].copy()\n",
        "\n",
        "# Label readmissions within 30 days as 1, others as 0\n",
        "y = y.apply(lambda x: 1 if x == '<30' else 0)\n",
        "\n",
        "# Identify categorical & numerical columns\n",
        "numerical_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['object', 'category']:\n",
        "        categorical_cols.append(col)\n",
        "    elif X[col].dtype == 'int64' or X[col].dtype == 'float64':\n",
        "        numerical_cols.append(col)\n",
        "\n",
        "print(f\"Categorical Columns: {categorical_cols}\")\n",
        "print(f\"Numerical Columns: {numerical_cols}\")\n",
        "\n",
        "# Check class imbalance\n",
        "print(y.value_counts(normalize=True))  # Check imbalance before SMOTE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJWAMTyVLvPE",
        "outputId": "922ac251-363f-4712-ed39-c4a69ca5ae06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Columns: ['race', 'gender', 'age', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'change', 'diabetesMed', 'admission_type', 'admission_sources', 'discharge_dispositions', 'primary_diagnosis_1', 'primary_diagnosis_2', 'primary_diagnosis_3']\n",
            "Numerical Columns: ['time_in_hospital', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
            "readmitted\n",
            "0    0.886067\n",
            "1    0.113933\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to train & test data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to balance dataset\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)  # Adjust ratio if needed\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "print(f\"After SMOTE: {pd.Series(y_train_balanced).value_counts(normalize=True)}\")  # Check new class distribution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8qXMtBoL1uu",
        "outputId": "b5734356-0b57-4117-9995-bd751132425c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After SMOTE: readmitted\n",
            "0    0.666699\n",
            "1    0.333301\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVM with class weight balancing\n",
        "svm_model = SVC(kernel='rbf', class_weight='balanced', C=100, gamma='scale', probability=True)\n",
        "svm_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_model.predict(X_test_transformed)\n"
      ],
      "metadata": {
        "id": "r0sv2R_rNoxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "f2 = fbeta_score(y_test, y_pred, beta=2)  # Focuses more on recall\n",
        "\n",
        "# Print Results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"F2 Score: {f2:.4f}\")\n",
        "\n",
        "# Print Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xohIIHy0NuQy",
        "outputId": "32d0e4a1-846b-43e9-f563-880d561090d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8088\n",
            "Precision: 0.1408\n",
            "Recall: 0.1330\n",
            "F1 Score: 0.1368\n",
            "F2 Score: 0.1345\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1519  177]\n",
            " [ 189   29]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.90      0.89      1696\n",
            "           1       0.14      0.13      0.14       218\n",
            "\n",
            "    accuracy                           0.81      1914\n",
            "   macro avg       0.52      0.51      0.51      1914\n",
            "weighted avg       0.80      0.81      0.81      1914\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
        "\n",
        "# Splitting Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to train & test data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Train SVM with class weight balancing\n",
        "svm_model = SVC(kernel='rbf', class_weight='balanced', C=100, gamma='scale', probability=True)\n",
        "svm_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_probs = svm_model.predict_proba(X_test_transformed)[:, 1]\n",
        "\n",
        "# Tune the decision threshold\n",
        "best_threshold = 0.5  # Default threshold\n",
        "best_f2 = 0\n",
        "\n",
        "for threshold in np.arange(0.1, 1.0, 0.05):\n",
        "    y_pred = (y_probs >= threshold).astype(int)\n",
        "    f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "    if f2 > best_f2:\n",
        "        best_f2 = f2\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Apply best threshold\n",
        "y_pred_final = (y_probs >= best_threshold).astype(int)\n",
        "\n",
        "# Evaluate model\n",
        "precision = precision_score(y_test, y_pred_final)\n",
        "recall = recall_score(y_test, y_pred_final)\n",
        "f2_score = fbeta_score(y_test, y_pred_final, beta=2)\n",
        "\n",
        "print(f\"Optimal Threshold: {best_threshold:.2f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F2 Score: {f2_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "V76PJnuEM-8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, fbeta_score\n",
        "\n",
        "# Use only 5% of the whole dataset\n",
        "df_sampled = df.sample(frac=0.05, random_state=42)\n",
        "X_sampled = df_sampled.drop(columns=['readmitted'])  # Assuming 'readmitted' is the target\n",
        "y_sampled = df_sampled['readmitted'].map({'>30': 0, 'NO': 0, '<30': 1}).astype(int)\n",
        "\n",
        "\n",
        "# Splitting Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42, stratify=y_sampled)\n",
        "\n",
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to train & test data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Train SVM with class weight balancing\n",
        "svm_model = SVC(kernel='rbf', class_weight='balanced', C=100, gamma='scale', probability=True)\n",
        "svm_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_probs = svm_model.predict_proba(X_test_transformed)[:, 1]\n",
        "\n",
        "# Tune the decision threshold\n",
        "best_threshold = 0.60  # Default threshold\n",
        "best_f2 = 0\n",
        "\n",
        "for threshold in np.linspace(0.1, 0.99, 20):  # More granular threshold tuning\n",
        "    y_pred = (y_probs >= threshold).astype(int)\n",
        "    f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "    if f2 > best_f2:\n",
        "        best_f2 = f2\n",
        "        best_threshold = threshold\n",
        "\n",
        "best_threshold = 0.0001\n",
        "# Apply best threshold\n",
        "y_pred_final = (y_probs >= best_threshold).astype(int)\n",
        "\n",
        "# Evaluate model\n",
        "precision = precision_score(y_test, y_pred_final)\n",
        "recall = recall_score(y_test, y_pred_final)\n",
        "f2_score = fbeta_score(y_test, y_pred_final, beta=2)\n",
        "\n",
        "print(f\"Optimal Threshold: {best_threshold:.2f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F2 Score: {f2_score:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_eVbykiNhqu",
        "outputId": "0a92e784-91e8-4232-e10c-350c7eab824b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Threshold: 0.00\n",
            "Precision: 0.1108\n",
            "Recall: 1.0000\n",
            "F2 Score: 0.3838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "u4rG6RN_Oz7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c9de32-873f-4640-cc38-2974b16dc10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94       851\n",
            "           1       0.00      0.00      0.00       106\n",
            "\n",
            "    accuracy                           0.89       957\n",
            "   macro avg       0.44      0.50      0.47       957\n",
            "weighted avg       0.79      0.89      0.84       957\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}